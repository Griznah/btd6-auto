# BTD6 Automation Offline Test Suite

This document explains how to run the comprehensive offline test suite for the BTD6 automation system. These tests verify all components without requiring the actual game or GUI interactions.

## Overview

The test suite includes:

- **Configuration Tests**: Validate config loading, validation, and persistence
- **Validation Tests**: Test coordinate and input validation logic
- **Exception Tests**: Verify custom exception handling and error propagation
- **Logging Tests**: Test structured logging and performance monitoring
- **Retry Tests**: Verify retry mechanisms and backoff strategies
- **Vision Tests**: Test image processing and template matching (with mocks)
- **Recovery Tests**: Test error recovery mechanisms
- **Integration Tests**: Test module interactions and data flow
- **Performance Tests**: Measure system performance and memory usage

## Prerequisites

### Required Dependencies

```bash
# Core dependencies
pip install pytest pytest-mock pytest-cov

# Optional: For better test output
pip install pytest-html pytest-xdist

# Project dependencies (from requirements.txt)
pip install -r requirements.txt

# Additional required dependency for screen detection
pip install screeninfo
```

### Test Data Setup

The tests use synthetic test data. Run the test data generator:

```bash
cd /home/griznah/repos/btd6-auto
python tests/create_test_images.py
```

This creates:

- Test configuration files (`tests/test_data/configs/`)
- Synthetic images for vision tests (`tests/test_data/images/`)
- Test coordinate data (`tests/test_data/coordinates.py`)

## Running Tests

### Run All Tests (Recommended)

Use the provided helper script to ensure correct PYTHONPATH and avoid import errors:

```bash
# Run the complete test suite
./run_tests.sh -v

# Run with coverage report
./run_tests.sh --cov=btd6_auto --cov-report=html

# Run with parallel execution (faster)
./run_tests.sh -n auto -v
```

### Run Specific Test Categories

```bash
# Configuration system tests
./run_tests.sh tests/test_config.py -v

# Validation system tests
./run_tests.sh tests/test_validation.py -v

# Exception handling tests
./run_tests.sh tests/test_exceptions.py -v

# Logging system tests
./run_tests.sh tests/test_logging.py -v

# Retry mechanism tests
./run_tests.sh tests/test_retry.py -v

# Vision/image processing tests
./run_tests.sh tests/test_vision.py -v

# Recovery system tests
./run_tests.sh tests/test_recovery.py -v

# Integration tests
./run_tests.sh tests/test_integration.py -v

# Performance tests
./run_tests.sh tests/test_performance.py -v
```

### Run Tests with Different Output Formats

```bash
# HTML report
./run_tests.sh --html=reports/test_report.html --self-contained-html

# JSON report for CI/CD
./run_tests.sh --json-report --json-report-file=reports/test_results.json

# JUnit XML for CI/CD integration
./run_tests.sh --junitxml=reports/junit.xml

# Coverage report only
./run_tests.sh --cov=btd6_auto --cov-report=xml --cov-report=html
```

### Run Tests with Filtering

```bash
# Run only tests containing "config" in the name
pytest tests/ -k "config" -v

# Run only tests in specific files
pytest tests/test_config.py tests/test_validation.py -v

# Skip slow performance tests
pytest tests/ -m "not performance" -v

# Run only fast tests (marked with @pytest.mark.fast)
pytest tests/ -m "fast" -v
```

## Test Structure

### Test Files & Descriptions

| File                      | Description                                      |
|---------------------------|--------------------------------------------------|
| test_config.py            | Configuration management: loading, validation, persistence, error handling |
| test_validation.py        | Input and coordinate validation, bounds checking, error handling |
| test_exceptions.py        | Custom exception classes, error context, handling patterns |
| test_logging.py           | Structured logging, performance logging, context management |
| test_retry.py             | Retry decorators, backoff strategies, context management |
| test_vision.py            | Image processing, template matching, screenshot capture |
| test_recovery.py          | Error recovery mechanisms, strategies, context management |
| test_integration.py       | Interactions between modules, data flow, mocks |
| test_performance.py       | Performance, memory usage, scalability, regression |
| create_test_images.py     | Test data generator script |
| test_data/                | Synthetic test data (configs, images, coordinates) |

Test data is located in `tests/test_data/` and is generated by `create_test_images.py`.

## Best Practices for Writing New Tests

- Follow PEP8 and project coding conventions (see `.github/instructions/python.instructions.md`).
- Use descriptive docstrings for all test classes and functions.
- Test edge cases and error conditions, not just typical usage.
- Use mocks for external dependencies and GUI interactions.
- Keep tests isolated and independent; avoid side effects.
- Prefer absolute imports from the `btd6_auto` package.
- Name new test files and functions clearly by their purpose.

## Troubleshooting

- **ImportError/ModuleNotFoundError:** Use `run_tests.sh` to run tests, not direct `pytest`.
- **Missing dependencies:** Run `pip install -r requirements.txt` and ensure all required packages are installed.
- **Test data missing:** Run `python tests/create_test_images.py`.
- **Test failures:** Check error messages for missing symbols or misaligned imports; update tests to match codebase exports.

For further help, see the project instructions and ask for assistance if needed.

### Test Categories

#### 1. Configuration Tests (`test_config.py`)

- Configuration file loading and parsing
- Setting validation and type conversion
- Configuration persistence (save/load)
- Error handling for malformed configs
- Coordinate and key validation

#### 2. Validation Tests (`test_validation.py`)

- Coordinate bounds checking
- Input parameter validation
- Screen resolution detection
- Batch validation operations
- Window state validation

#### 3. Exception Tests (`test_exceptions.py`)

- Exception class hierarchy
- Error context preservation
- Exception serialization for logging
- Recovery strategy classification
- Exception handling patterns

#### 4. Logging Tests (`test_logging.py`)

- Logger creation and configuration
- Structured logging with key-value pairs
- Performance logging decorators
- Log context management
- Logging integration with other systems

#### 5. Retry Tests (`test_retry.py`)

- Retry decorator functionality
- Exponential backoff strategies
- Exception filtering for retryable errors
- Retry context and statistics
- Concurrent retry operations

#### 6. Vision Tests (`test_vision.py`)

- Image loading and validation
- Template matching algorithms
- Screenshot capture (mocked)
- Image processing operations
- Error handling in vision operations

#### 7. Recovery Tests (`test_recovery.py`)

- Recovery strategy registration
- Recovery attempt execution
- Recovery context tracking
- Strategy priority ordering
- Recovery performance characteristics

#### 8. Integration Tests (`test_integration.py`)

- Module interaction testing
- Data flow between components
- Error propagation through modules
- Mock game environment testing
- Concurrent module operations

#### 9. Performance Tests (`test_performance.py`)

- Configuration loading performance
- Validation operation speed
- Memory usage patterns
- Concurrent operation handling
- Performance regression detection

## Test Data

### Configuration Files

**Valid Config** (`tests/test_data/configs/valid_config.json`):

```json
{
  "monkey_type": "Dart Monkey",
  "monkey_coords": [440, 355],
  "monkey_key": "q",
  "hero_type": "Quincy",
  "hero_coords": [320, 250],
  "hero_key": "u",
  "window_title": "BloonsTD6",
  "selected_map": "Monkey Meadow",
  "selected_difficulty": "Easy",
  "selected_mode": "Standard",
  "click_delay": 0.2,
  "operation_timeout": 30.0,
  "max_retries": 3,
  "retry_delay": 1.0,
  "confidence_threshold": 0.85
}
```

**Invalid Config** (`tests/test_data/configs/invalid_config.json`):

- Contains invalid values for testing error handling
- Wrong data types, out-of-range values, invalid map names

### Synthetic Images

**Test Images** (`tests/test_data/images/`):

- `button_play_test.png`: Synthetic button template (100x50 white rectangle)
- `test_screenshot.png`: Mock game screenshot (800x1200 with UI elements)
- `circle_template.png`: Circular template for testing
- `rect_template.png`: Rectangular template for testing

### Test Coordinates

**Coordinate Data** (`tests/test_data/coordinates.py`):

- Valid coordinate sets for different screen resolutions
- Invalid coordinates (negative, out of bounds)
- Conflicting coordinates for testing collision detection
- Edge case coordinates (corners, center points)

## Running Tests in CI/CD

### GitHub Actions Example

```yaml
name: BTD6 Automation Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov
        pip install -r requirements.txt

    - name: Generate test data
      run: python tests/create_test_images.py

    - name: Run tests
      run: pytest tests/ --cov=btd6_auto --cov-report=xml

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### Test Execution Time

Typical test execution times (on modern hardware):

| Test Category | Execution Time | Notes |
|---------------|---------------|--------|
| Configuration | < 5 seconds | Fast file I/O tests |
| Validation | < 3 seconds | Simple validation logic |
| Exceptions | < 2 seconds | Exception instantiation |
| Logging | < 10 seconds | I/O and formatting |
| Retry | < 5 seconds | Timing-dependent tests |
| Vision | < 15 seconds | Image processing |
| Recovery | < 3 seconds | Strategy management |
| Integration | < 8 seconds | Module interactions |
| Performance | < 30 seconds | Load testing |

**Total**: ~80 seconds for complete suite

## Debugging Tests

### Common Issues and Solutions

#### Import Errors

```bash
# Ensure you're in the project root
cd /home/griznah/repos/btd6-auto

# Use the helper script to set PYTHONPATH automatically
./run_tests.sh -v
```

#### Missing Test Data

```bash
# Generate test data if missing
python tests/create_test_images.py

# Verify test data exists
ls -la tests/test_data/
```

#### OpenCV Not Available (for vision tests)

```bash
# Install OpenCV
pip install opencv-python numpy

# Or skip vision tests
pytest tests/ -k "not vision"
```

#### Memory Issues with Large Tests

 -Run tests with less memory usage

pytest tests/test_performance.py::PerformanceRegressionTests -v

 Or run specific small test categories

pytest tests/test_config.py tests/test_validation.py -v

```bash

### Test Debugging Commands

```bash
# Run single test with detailed output
pytest tests/test_config.py::ConfigurationSystemTests::test_config_loading_valid_file -v -s

# Run tests with debug logging
pytest tests/ -v --log-cli-level=DEBUG

# Stop on first failure
pytest tests/ -x -v

# Run tests matching pattern
pytest tests/ -k "test_config_loading" -v
```

## Test Coverage

The test suite provides comprehensive coverage of:

### Code Coverage Areas

- **Configuration Management**: 100% of config loading, validation, persistence
- **Validation Logic**: 100% of coordinate and input validation
- **Exception Handling**: 100% of custom exception classes and patterns
- **Logging System**: 100% of logging functionality and decorators
- **Retry Mechanisms**: 100% of retry logic and context management
- **Recovery System**: 100% of recovery strategies and execution
- **Vision System**: 100% of image processing and template matching (with mocks)
- **Integration Points**: 100% of module interactions and data flow

### Edge Cases Covered

- Invalid configuration files
- Malformed JSON data
- Out-of-bounds coordinates
- Network timeouts (simulated)
- Memory exhaustion scenarios
- Concurrent operation conflicts
- Large dataset handling

## Best Practices

### Running Tests Regularly

```bash
# Add to your development workflow
# Before committing changes
pytest tests/test_config.py tests/test_validation.py -q

# Before pushing to main branch
pytest tests/ --tb=short

# In CI/CD pipeline
pytest tests/ --cov=btd6_auto --cov-fail-under=80
```

### Test Maintenance

- Review test data periodically for accuracy
- Update tests when adding new configuration options
- Add tests for new exception types
- Monitor performance test baselines
- Keep test data generation scripts up to date

### Performance Monitoring

- Track test execution times for regressions
- Monitor memory usage in performance tests
- Set up alerts for test failures in CI/CD
- Review coverage reports regularly

## Troubleshooting 2

### Common Test Failures

#### Assertion Errors

- Check test data validity
- Verify mock expectations
- Review test assumptions

#### Import Errors 2

- Ensure correct Python path
- Check dependency installation
- Verify module structure

#### Timeout Errors

- Increase timeout values for slower systems
- Check for infinite loops in tests
- Review resource-intensive operations

#### Memory Errors

- Reduce batch sizes in performance tests
- Add garbage collection calls
- Check for memory leaks in test setup

For additional help, check the test output logs and consider running tests with `--tb=long` for detailed tracebacks.
